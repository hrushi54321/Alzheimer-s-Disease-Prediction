{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QG29peNJwAyC",
        "lWu5ScFfwJJR",
        "KDkNCYWGwP20",
        "GYIxudB0Z_La",
        "nlRGS39ByRqm",
        "qdRGWOcpyWfA",
        "aahIODGrygPC"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-CqEGO5wB9r"
      },
      "source": [
        "#Imports and Uploads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsXz0ETdv2CU"
      },
      "source": [
        "'''\n",
        "The four files need to be uploaded onto the colab environment:\n",
        "COG.csv, PHENO.csv, MRI.csv, DEMO.csv\n",
        "It can be uploaded by running this cell\n",
        "'''\n",
        "from google.colab import files\n",
        "m,p,d,i = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdwLuhNyv5-8"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import GRU\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.impute import KNNImputer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBCn52J7v7fy"
      },
      "source": [
        "#Once the files are uploaded onto the environment\n",
        "pheno = pd.read_csv('PHENO.csv')\n",
        "mri = pd.read_csv('MRI.csv')\n",
        "demo = pd.read_csv('DEMO.csv')\n",
        "cog = pd.read_csv('COG.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG29peNJwAyC"
      },
      "source": [
        "#Correlation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koK7jczsv9Pi"
      },
      "source": [
        "cog = pd.read_csv('COG.csv')\n",
        "mri = pd.read_csv('MRI.csv')\n",
        "pheno = pd.read_csv('PHENO.csv')\n",
        "demo = pd.read_csv('DEMO.csv')\n",
        "\n",
        "co = list(cog.columns)\n",
        "mr = list(mri.columns)\n",
        "ph = list(pheno.columns)\n",
        "de = list(demo.columns)\n",
        "\n",
        "myset = {'Unnamed: 0','RID','VISCODE','DX_bl','PTGENDER'}\n",
        "co = [ele for ele in co if ele not in myset]\n",
        "mr = [ele for ele in mr if ele not in myset]\n",
        "ph = [ele for ele in ph if ele not in myset]\n",
        "de = [ele for ele in de if ele not in myset]\n",
        "\n",
        "print(\"Within modal correlation:\\n\")\n",
        "\n",
        "coeff, _ = pearsonr(cog['EcogPtTotal'],cog['EcogPtMem'])\n",
        "print(\"Between \",co[0],\" and \",co[1],\" = \",coeff)\n",
        "coeff, _ = pearsonr(mri['Hippocampus'],mri['Entorhinal'])\n",
        "print(\"Between \",mr[0],\" and \",mr[1],\" = \",coeff)\n",
        "for n in range(len(ph)):\n",
        "\tfor m in range(n+1,len(ph)):\n",
        "\t\tcoeff, _ = pearsonr(pheno[ph[n]],pheno[ph[m]])\n",
        "\t\tprint(\"Between \",ph[n],\" and \",ph[m],\" = \",coeff)\n",
        "for n in range(len(de)):\n",
        "\tfor m in range(n+1,len(de)):\n",
        "\t\tcoeff, _ = pearsonr(demo[de[n]],demo[de[m]])\n",
        "\t\tprint(\"Between \",de[n],\" and \",de[m],\" = \",coeff)\n",
        "\n",
        "print(\"\\nBetween modal correlation:\\n\")\n",
        "\n",
        "mer = pd.merge(cog[['RID','EcogPtMem','EcogPtTotal']], mri[['RID','Hippocampus','Entorhinal']], how = 'inner')\n",
        "for n in range(len(co)):\n",
        "\tfor m in range(len(mr)):\n",
        "\t\tcoeff, _ = pearsonr(mer[co[n]],mer[mr[m]])\n",
        "\t\tprint(\"Between \",co[n],\" and \",mr[m],\" = \",coeff)\n",
        "\n",
        "mer = pd.merge(cog[['RID','EcogPtMem','EcogPtTotal']], pheno[['RID','ABETA','TAU','PTAU']], how = 'inner')\n",
        "for n in range(len(co)):\n",
        "\tfor m in range(len(ph)):\n",
        "\t\tcoeff, _ = pearsonr(mer[co[n]],mer[ph[m]])\n",
        "\t\tprint(\"Between \",co[n],\" and \",ph[m],\" = \",coeff)\n",
        "\n",
        "mer = pd.merge(cog[['RID','EcogPtMem','EcogPtTotal']], demo[['RID','AGE','PTEDUCAT','APOE4']], how = 'inner')\n",
        "for n in range(len(co)):\n",
        "\tfor m in range(len(de)):\n",
        "\t\tcoeff, _ = pearsonr(mer[co[n]],mer[de[m]])\n",
        "\t\tprint(\"Between \",co[n],\" and \",de[m],\" = \",coeff)\n",
        "\n",
        "mer = pd.merge(mri[['RID','Hippocampus','Entorhinal']], pheno[['RID','ABETA','TAU','PTAU']])\n",
        "for n in range(len(mr)):\n",
        "\tfor m in range(len(ph)):\n",
        "\t\tcoeff, _ = pearsonr(mer[mr[n]],mer[ph[m]])\n",
        "\t\tprint(\"Between \",mr[n],\" and \",ph[m],\" = \",coeff)\n",
        "\n",
        "mer = pd.merge(mri[['RID','Hippocampus','Entorhinal']], demo[['RID','AGE','PTEDUCAT','APOE4']])\n",
        "for n in range(len(mr)):\n",
        "\tfor m in range(len(de)):\n",
        "\t\tcoeff, _ = pearsonr(mer[mr[n]],mer[de[m]])\n",
        "\t\tprint(\"Between \",mr[n],\" and \",de[m],\" = \",coeff)\n",
        "\n",
        "mer = pd.merge(pheno[['RID','ABETA','TAU','PTAU']], demo[['RID','AGE','PTEDUCAT','APOE4']])\n",
        "for n in range(len(ph)):\n",
        "\tfor m in range(len(de)):\n",
        "\t\tcoeff, _ = pearsonr(mer[ph[n]],mer[de[m]])\n",
        "\t\tprint(\"Between \",ph[n],\" and \",de[m],\" = \",coeff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWu5ScFfwJJR"
      },
      "source": [
        "#Extracting Feature Vectors:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YURIL2DSwJ7C"
      },
      "source": [
        "##Cog:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZV8TiozwMQB"
      },
      "source": [
        "#Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILHaNfEowSyp"
      },
      "source": [
        "cog = cog.sort_values(by = ['RID'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDkNCYWGwP20"
      },
      "source": [
        "###Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxE2K-3IwUaz"
      },
      "source": [
        "sns.catplot(x='VISCODE', y='EcogPtMem', data=cog, hue=\"DX_bl\", height=8.27, aspect=11.7/8.27)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NCEbFbPwV2k"
      },
      "source": [
        "sns.catplot(x='VISCODE', y='EcogPtTotal', data=cog, hue=\"DX_bl\", height=8.27, aspect=11.7/8.27)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsMNCjbKwYeq"
      },
      "source": [
        "###Data formatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrSg_0XiwZSh"
      },
      "source": [
        "'''\n",
        "Formatting the data as well as removing the data which contain too many missing \n",
        "values.\n",
        "'''\n",
        "CogPtMem = []\n",
        "CogPtTotal = []\n",
        "labels = []\n",
        "start = 2\n",
        "rids_cog = []\n",
        "l1 = [0,0,0,0,0,0] #bl, m06, m12, m24, m36, m48 PTMEM\n",
        "l2 = [0,0,0,0,0,0] #bl, m06, m12, m24, m36, m48 PTTOT\n",
        "cnt = 0\n",
        "for index,row in cog.iterrows():\n",
        "  if start != row['RID']:    \n",
        "    if sum(l1) != 0 and sum(l2) != 0 and cnt>2:\n",
        "      rids_cog.append(row['RID'])\n",
        "      CogPtMem.append(l1)\n",
        "      CogPtTotal.append(l2)\n",
        "      if dec == 'AD':\n",
        "        labels.append(1)\n",
        "      else:\n",
        "        labels.append(0)\n",
        "    l1 = [0,0,0,0,0,0]\n",
        "    l2 = [0,0,0,0,0,0]\n",
        "    start = row['RID']\n",
        "    cnt=0\n",
        "  dec = row['DX_bl']\n",
        "  m = row['VISCODE']\n",
        "  ptm = row['EcogPtMem']\n",
        "  ptt = row['EcogPtTotal']\n",
        "  if m == 'bl':\n",
        "    cnt+=1\n",
        "    l1[0] = ptm\n",
        "    l2[0] = ptt\n",
        "  elif m == 'm06':\n",
        "    cnt+=1\n",
        "    l1[1] = ptm\n",
        "    l2[1] = ptt\n",
        "  elif m == 'm12':\n",
        "    cnt+=1\n",
        "    l1[2] = ptm\n",
        "    l2[2] = ptt\n",
        "  elif m == 'm24':\n",
        "    cnt+=1\n",
        "    l1[3] = ptm\n",
        "    l2[3] = ptt\n",
        "  elif m == 'm36':\n",
        "    cnt+=1\n",
        "    l1[4] = ptm\n",
        "    l2[4] = ptt\n",
        "  elif m == 'm48':\n",
        "    cnt+=1\n",
        "    l1[5] = ptm\n",
        "    l2[5] = ptt\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "796FkiSxBdNR"
      },
      "source": [
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.datasets import make_classification\n",
        "from collections import Counter\n",
        "from numpy import where\n",
        "from matplotlib import pyplot\n",
        "# define dataset\n",
        "X_smote = np.concatenate((np.array(CogPtMem), np.array(CogPtTotal)), axis=1)\n",
        "y_smote = labels\n",
        "\n",
        "print(X_smote)\n",
        "print(y_smote)\n",
        "\n",
        "# summarize class distribution\n",
        "counter = Counter(y_smote)\n",
        "print(counter)\n",
        "\n",
        "# scatter plot of examples by class label\n",
        "for label, _ in counter.items():\n",
        "\trow_ix = where(y_smote == label)[0]\n",
        "\tpyplot.scatter(X_smote[row_ix, 0], X_smote[row_ix, 1], label=str(label))\n",
        "pyplot.legend()\n",
        "pyplot.show()\n",
        "\n",
        "# transform the dataset\n",
        "oversample = SMOTE()\n",
        "X_smote, y_smote = oversample.fit_resample(X_smote, y_smote)\n",
        "\n",
        "print(X_smote)\n",
        "print(y_smote)\n",
        "\n",
        "# summarize the new class distribution\n",
        "counter = Counter(y_smote)\n",
        "print(counter)\n",
        "\n",
        "# scatter plot of examples by class label\n",
        "for label, _ in counter.items():\n",
        "\trow_ix = where(y_smote == label)[0]\n",
        "\tpyplot.scatter(X_smote[row_ix, 0], X_smote[row_ix, 1], label=str(label))\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSinV7WLwdAq"
      },
      "source": [
        "newarr = np.array_split(X_smote, 2, axis=1)\n",
        "x1 = newarr[0]\n",
        "x2 = newarr[1]\n",
        "#Length of data in Cog\n",
        "print(len(x1),len(x2))\n",
        "y_cog = y_smote"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDRWYdcUwgcq"
      },
      "source": [
        "#KNN Imputation is done to fill in the few missing values which are left\n",
        "imputer = KNNImputer(n_neighbors=28, missing_values=0, weights = 'distance')\n",
        "x1_cog = imputer.fit_transform(x1)  #CogPtMem\n",
        "x2_cog = imputer.fit_transform(x2)  #CogPtTotal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZoYvR7wwipC"
      },
      "source": [
        "###Model_cog"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z41EbCn_wkqC"
      },
      "source": [
        "print(\"Shape of input:\",len(x1_cog),len(x1_cog[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ywk2q8kEwn_6"
      },
      "source": [
        "#GRUs require 3 dimensional input\n",
        "x1_cog = x1_cog.reshape((1,1430,6))\n",
        "x2_cog = x2_cog.reshape((1,1430,6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coLU9tAgwqor"
      },
      "source": [
        "#Generating feature vectors\n",
        "inputs1 = Input(shape=(1430, 6))\n",
        "gru1 = GRU(6, return_sequences=True, return_state=True)\n",
        "whole_sequence_output, final_state = gru1(inputs1)\n",
        "model1 = Model(inputs=inputs1, outputs=whole_sequence_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGSbXI0DwtG7"
      },
      "source": [
        "cogptmem_fv = model1.predict(x1_cog) #CogPtMem: fv = feature vector\n",
        "cogpttotal_fv = model1.predict(x2_cog) #CogPtTotal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwSOUScCwtrq"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# 70% training and 30% test\n",
        "CogPtMem_1 = cogptmem_fv[0]\n",
        "cogpttotal_1 = cogpttotal_fv[0]\n",
        "cog_fv = np.concatenate((CogPtMem_1, cogpttotal_1), axis=1)\n",
        "print(y_cog)\n",
        "print(cog_fv.shape)\n",
        "X_train, X_test, y_train, y_test = train_test_split(cog_fv, y_cog, test_size=0.3,random_state=109)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ViGHsjvZLKg"
      },
      "source": [
        "#Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHXkK-q5ZArp"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# Create the model with 100 trees\n",
        "model = RandomForestClassifier(n_estimators=100, \n",
        "                               bootstrap = True,\n",
        "                               max_features = 'sqrt')\n",
        "# Fit on training data\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APyccHU9ZFy1"
      },
      "source": [
        "# Actual class predictions\n",
        "rf_predictions = model.predict(X_test)\n",
        "# Probabilities for each class\n",
        "rf_probs = model.predict_proba(X_test)[:, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XDs9coxZIUZ"
      },
      "source": [
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score, cohen_kappa_score\n",
        "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, permutation_test_score\n",
        "\n",
        "# Model Accuracy: how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, rf_predictions))\n",
        "\n",
        "# Model Precision: what percentage of positive tuples are labeled as such?\n",
        "print(\"Precision:\",metrics.precision_score(y_test, rf_predictions))\n",
        "\n",
        "# Model Recall: what percentage of positive tuples are labelled as such?\n",
        "print(\"Recall:\",metrics.recall_score(y_test, rf_predictions))\n",
        "\n",
        "#Printing Confusion Matrix\n",
        "conf_matrix_rf = metrics.confusion_matrix(y_test, rf_predictions)\n",
        "print(\"Confusion Matrix: \", conf_matrix_rf )\n",
        "\n",
        "#Printing F1 Score\n",
        "f1score = f1_score(y_test, rf_predictions, average='binary')\n",
        "print(\"F1 score: \", f1score)\n",
        "\n",
        "#Printing Cohen-Kappa score\n",
        "ckscore = cohen_kappa_score(y_test, rf_predictions)\n",
        "print(\"Cohen-Kappa Score: \", ckscore)\n",
        "\n",
        "#Printing Precision-Recall Curve\n",
        "average_precision = average_precision_score(y_test, rf_predictions)\n",
        "print(\"Average Precision: \", average_precision)\n",
        "disp = plot_precision_recall_curve(model, X_test, y_test)\n",
        "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
        "                   'AP={0:0.2f}'.format(average_precision))\n",
        "\n",
        "\n",
        "#Printing ROC Curve\n",
        "metrics.plot_roc_curve(model, X_test, y_test)\n",
        "plt.show()\n",
        "\n",
        "#P-value\n",
        "_, _, p_value = permutation_test_score(model, X_test, y_test, scoring=\"neg_root_mean_squared_error\", cv=None, n_permutations=200)\n",
        "\n",
        "print(p_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ISI53zzyvfki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE3EoJKcZQRG"
      },
      "source": [
        "#SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1tfaT9dyWiN"
      },
      "source": [
        "from sklearn import svm\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#Create a svm Classifier\n",
        "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
        "#Train the model using the training sets\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred1 = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjNWQSTMLyPR"
      },
      "source": [
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "\n",
        "# Model Accuracy: how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred1))\n",
        "\n",
        "# Model Precision: what percentage of positive tuples are labeled as such?\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred1))\n",
        "\n",
        "# Model Recall: what percentage of positive tuples are labelled as such?\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred1))\n",
        "\n",
        "#Printing Confusion Matrix\n",
        "conf_matrix_svm = metrics.confusion_matrix(y_test, y_pred1)\n",
        "print(\"Confusion Matrix: \", conf_matrix_svm )\n",
        "\n",
        "#Printing F1 Score\n",
        "f1score = f1_score(y_test, y_pred1, average='binary')\n",
        "print(\"F1 score: \", f1score)\n",
        "\n",
        "#Printing Cohen-Kappa score\n",
        "ckscore = cohen_kappa_score(y_test, y_pred1)\n",
        "print(\"Cohen-Kappa Score: \", ckscore)\n",
        "\n",
        "#Printing Precision-Recall Curve\n",
        "average_precision = average_precision_score(y_test, y_pred1)\n",
        "disp = plot_precision_recall_curve(clf, X_test, y_test)\n",
        "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
        "                   'AP={0:0.2f}'.format(average_precision))\n",
        "\n",
        "\n",
        "#Printing ROC Curve\n",
        "metrics.plot_roc_curve(clf, X_test, y_test)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#P-value\n",
        "_, _, p_value = permutation_test_score(clf, X_test, y_test, scoring=\"neg_root_mean_squared_error\", cv=None, n_permutations=100)\n",
        "print(p_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTuHpUSD7caK"
      },
      "source": [
        "#Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd7VaHld7fII"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.tree import export_graphviz \n",
        "from IPython.display import Image \n",
        "from pydot import graph_from_dot_data\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Msbr4qNrK7Fx"
      },
      "source": [
        "y_pred = dt.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSzKAKLW7rvB"
      },
      "source": [
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "\n",
        "# Model Accuracy: how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Model Precision: what percentage of positive tuples are labeled as such?\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
        "\n",
        "# Model Recall: what percentage of positive tuples are labelled as such?\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
        "\n",
        "#Printing Confusion Matrix\n",
        "conf_matrix_dt = metrics.confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix: \", conf_matrix_dt )\n",
        "\n",
        "#Printing F1 Score\n",
        "f1score = f1_score(y_test, y_pred, average='binary')\n",
        "print(\"F1 score: \", f1score)\n",
        "\n",
        "#Printing Cohen-Kappa score\n",
        "ckscore = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Cohen-Kappa Score: \", ckscore)\n",
        "\n",
        "#Printing Precision-Recall Curve\n",
        "average_precision = average_precision_score(y_test, y_pred)\n",
        "disp = plot_precision_recall_curve(dt, X_test, y_test)\n",
        "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
        "                   'AP={0:0.2f}'.format(average_precision))\n",
        "\n",
        "\n",
        "#Printing ROC Curve\n",
        "metrics.plot_roc_curve(dt, X_test, y_test)\n",
        "plt.show()\n",
        "\n",
        "#P-value\n",
        "_, _, p_value = permutation_test_score(dt, X_test, y_test, scoring=\"neg_root_mean_squared_error\", cv=None, n_permutations=200)\n",
        "print(p_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ensemble Model"
      ],
      "metadata": {
        "id": "uSMHsgRwihoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import six\n",
        "import sys\n",
        "sys.modules['sklearn.externals.six'] = six\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "lr = LogisticRegression()\n",
        "clf_stack = StackingClassifier(classifiers =[clf, dt, model], meta_classifier = lr, use_probas = False, use_features_in_secondary = True)\n",
        "model_stack = clf_stack.fit(X_train, y_train)\n",
        "pred_stack = model_stack.predict(X_test)"
      ],
      "metadata": {
        "id": "Pn_5edBxik4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "\n",
        "# Model Accuracy: how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, pred_stack))\n",
        "\n",
        "# Model Precision: what percentage of positive tuples are labeled as such?\n",
        "print(\"Precision:\",metrics.precision_score(y_test, pred_stack))\n",
        "\n",
        "# Model Recall: what percentage of positive tuples are labelled as such?\n",
        "print(\"Recall:\",metrics.recall_score(y_test, pred_stack))\n",
        "\n",
        "#Printing Confusion Matrix\n",
        "conf_matrix_dt = metrics.confusion_matrix(y_test, pred_stack)\n",
        "print(\"Confusion Matrix: \", conf_matrix_dt )\n",
        "\n",
        "#Printing F1 Score\n",
        "f1score = f1_score(y_test, pred_stack, average='binary')\n",
        "print(\"F1 score: \", f1score)\n",
        "\n",
        "#Printing Cohen-Kappa score\n",
        "ckscore = cohen_kappa_score(y_test, pred_stack)\n",
        "print(\"Cohen-Kappa Score: \", ckscore)\n",
        "\n",
        "#Printing Precision-Recall Curve\n",
        "average_precision = average_precision_score(y_test, pred_stack)\n",
        "disp = plot_precision_recall_curve(model_stack, X_test, y_test)\n",
        "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
        "                   'AP={0:0.2f}'.format(average_precision))\n",
        "\n",
        "\n",
        "#Printing ROC Curve\n",
        "metrics.plot_roc_curve(model_stack, X_test, y_test)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RhW8vO0zjB88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Making Combined Graphs of models."
      ],
      "metadata": {
        "id": "uHujYKfaWcXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ROC Curve\n",
        "\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "from sklearn.metrics import plot_roc_curve\n",
        "classifiers = [clf, dt, model]\n",
        "ax = plt.gca()\n",
        "for i in classifiers:\n",
        "    plot_roc_curve(i, X_test, y_test, ax=ax)"
      ],
      "metadata": {
        "id": "FH_3EHu3WxfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Precision Recall Curve\n",
        "\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "classifiers = [clf, dt, model]\n",
        "ax = plt.gca()\n",
        "for i in classifiers:\n",
        "    plot_precision_recall_curve(i, X_test, y_test, ax=ax)"
      ],
      "metadata": {
        "id": "BoxtzLqxbKG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHesEa8yZxN0"
      },
      "source": [
        "##Pheno:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHxjqFTIZ1TK"
      },
      "source": [
        "#dataset\n",
        "pheno = pheno.sort_values(by = ['RID'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o97TLvh8Z2KK"
      },
      "source": [
        "sns.catplot(x='VISCODE', y='ABETA', data=pheno, hue=\"DX_bl\", height=8.27, aspect=11.7/8.27)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLDFpeZCZ4WU"
      },
      "source": [
        "sns.catplot(x='VISCODE', y='TAU', data=pheno, hue=\"DX_bl\", height=8.27, aspect=11.7/8.27)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bEzdSxqZ820"
      },
      "source": [
        "sns.catplot(x='VISCODE', y='PTAU', data=pheno, hue=\"DX_bl\", height=8.27, aspect=11.7/8.27)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYIxudB0Z_La"
      },
      "source": [
        "###Data formatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0Wim2xpaAGV"
      },
      "source": [
        "'''\n",
        "Formatting the data as well as removing the data which contain too many missing \n",
        "values.\n",
        "'''\n",
        "abeta = []\n",
        "tau = []\n",
        "ptau = []\n",
        "labels = []\n",
        "start = 3\n",
        "rids_pheno = []\n",
        "l1 = [0,0,0,0,0,0] #bl, m06, m12, m24, m36, m48 abeta\n",
        "l2 = [0,0,0,0,0,0] #bl, m06, m12, m24, m36, m48 tau\n",
        "l3 = [0,0,0,0,0,0] #bl, m06, m12, m24, m36, m48 ptau\n",
        "cnt = 0\n",
        "for index,row in pheno.iterrows():\n",
        "  if start != row['RID']:    \n",
        "    if sum(l1) != 0 and sum(l2) != 0 and sum(l3) != 0 and cnt>2:\n",
        "      rids_pheno.append(row['RID'])\n",
        "      abeta.append(l1)\n",
        "      tau.append(l2)\n",
        "      ptau.append(l3)\n",
        "      if dec == 'AD':\n",
        "        labels.append(1)\n",
        "      else:\n",
        "        labels.append(0)\n",
        "    l1 = [0,0,0,0,0,0]\n",
        "    l2 = [0,0,0,0,0,0]\n",
        "    l3 = [0,0,0,0,0,0]\n",
        "    start = row['RID']\n",
        "    cnt=0\n",
        "  dec = row['DX_bl']\n",
        "  m = row['VISCODE']\n",
        "  abt = row['ABETA']\n",
        "  ta = row['TAU']\n",
        "  pta = row['PTAU']\n",
        "  if m == 'bl':\n",
        "    cnt+=1\n",
        "    l1[0] = abt\n",
        "    l2[0] = ta\n",
        "    l3[0] = pta\n",
        "  elif m == 'm06':\n",
        "    cnt+=1\n",
        "    l1[1] = abt\n",
        "    l2[1] = ta\n",
        "    l3[1] = pta\n",
        "  elif m == 'm12':\n",
        "    cnt+=1\n",
        "    l1[2] = abt\n",
        "    l2[2] = ta\n",
        "    l3[2] = pta\n",
        "  elif m == 'm24':\n",
        "    cnt+=1\n",
        "    l1[3] = abt\n",
        "    l2[3] = ta\n",
        "    l3[3] = pta\n",
        "  elif m == 'm36':\n",
        "    cnt+=1\n",
        "    l1[4] = abt\n",
        "    l2[4] = ta\n",
        "    l3[4] = pta\n",
        "  elif m == 'm48':\n",
        "    cnt+=1\n",
        "    l1[5] = abt\n",
        "    l2[5] = ta\n",
        "    l3[5] = pta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vqMx5hrc_bD"
      },
      "source": [
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.datasets import make_classification\n",
        "from collections import Counter\n",
        "from numpy import where\n",
        "from matplotlib import pyplot\n",
        "# define dataset\n",
        "X_smote = np.concatenate((np.array(abeta), np.array(tau), np.array(ptau)), axis=1)\n",
        "y_smote = labels\n",
        "\n",
        "print(X_smote)\n",
        "print(y_smote)\n",
        "\n",
        "# summarize class distribution\n",
        "counter = Counter(y_smote)\n",
        "print(counter)\n",
        "\n",
        "# scatter plot of examples by class label\n",
        "for label, _ in counter.items():\n",
        "\trow_ix = where(y_smote == label)[0]\n",
        "\tpyplot.scatter(X_smote[row_ix, 0], X_smote[row_ix, 1], label=str(label))\n",
        "pyplot.legend()\n",
        "pyplot.show()\n",
        "\n",
        "# transform the dataset\n",
        "oversample = SMOTE(sampling_strategy=0.5)\n",
        "X_smote, y_smote = oversample.fit_resample(X_smote, y_smote)\n",
        "\n",
        "print(X_smote)\n",
        "print(y_smote)\n",
        "\n",
        "# summarize the new class distribution\n",
        "counter = Counter(y_smote)\n",
        "print(counter)\n",
        "\n",
        "# scatter plot of examples by class label\n",
        "for label, _ in counter.items():\n",
        "\trow_ix = where(y_smote == label)[0]\n",
        "\tpyplot.scatter(X_smote[row_ix, 0], X_smote[row_ix, 1], label=str(label))\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WX-jpoAdEzt"
      },
      "source": [
        "newarr = np.array_split(X_smote, 3, axis=1)\n",
        "x1 = newarr[0]\n",
        "x2 = newarr[1]\n",
        "x3 = newarr[2]\n",
        "#Length of data in Pheno\n",
        "y_pheno = y_smote"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90oY29V6aI_K"
      },
      "source": [
        "#Imputation\n",
        "imputer = KNNImputer(n_neighbors=16, missing_values=0, weights = 'distance')\n",
        "x1_pheno = imputer.fit_transform(x1) #ABETA\n",
        "x2_pheno = imputer.fit_transform(x2) #TAU\n",
        "x3_pheno = imputer.fit_transform(x3) #PTAU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVFJAGTlaEmM"
      },
      "source": [
        "###Model_pheno"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsKUKe9GaO3U"
      },
      "source": [
        "print(\"Shape of input:\",len(x1_pheno),len(x1_pheno[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6IfTACiaTl9"
      },
      "source": [
        "#Reshaping for GRU\n",
        "x1_pheno = x1_pheno.reshape((1,337,6))\n",
        "x2_pheno = x2_pheno.reshape((1,337,6))\n",
        "x3_pheno = x3_pheno.reshape((1,337,6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP0QAsZ1aVW5"
      },
      "source": [
        "#Normalizing all data\n",
        "x1_pheno = x1_pheno/400\n",
        "x2_pheno = x2_pheno/200\n",
        "x3_pheno = x3_pheno/50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgdPrzuXaXY0"
      },
      "source": [
        "#Generating feature vectors\n",
        "inputs2 = Input(shape=(337, 6))\n",
        "gru2 = GRU(6, return_sequences=True, return_state=True)\n",
        "whole_sequence_output, final_state = gru2(inputs2)\n",
        "model2 = Model(inputs=inputs2, outputs=whole_sequence_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SFfXV6waZbL"
      },
      "source": [
        "abeta_fv = model2.predict(x1_pheno)\n",
        "tau_fv = model2.predict(x2_pheno)\n",
        "ptau_fv = model2.predict(x3_pheno)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CGaWWN6ZxTp"
      },
      "source": [
        "#Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0MGhLoXZz3J"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# 70% training and 30% test\n",
        "y_pheno = y_smote\n",
        "abeta_fv1 = abeta_fv[0]\n",
        "tau_fv1 = tau_fv[0]\n",
        "ptau_fv1 = ptau_fv[0]\n",
        "pheno_fv = np.concatenate((abeta_fv1, tau_fv1, ptau_fv1), axis=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(pheno_fv, y_pheno, test_size=0.3,random_state=109)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJBScZz-Z3p4"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# Create the model with 100 trees\n",
        "model = RandomForestClassifier(n_estimators=100, \n",
        "                               bootstrap = True,\n",
        "                               max_features = 'sqrt')\n",
        "# Fit on training data\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hpC-X2wZ6lg"
      },
      "source": [
        "# Actual class predictions\n",
        "rf_predictions = model.predict(X_test)\n",
        "# Probabilities for each class\n",
        "rf_probs = model.predict_proba(X_test)[:, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kml1zgaTZ81A"
      },
      "source": [
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split, permutation_test_score\n",
        "\n",
        "# Model Accuracy: how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, rf_predictions))\n",
        "\n",
        "# Model Precision: what percentage of positive tuples are labeled as such?\n",
        "print(\"Precision:\",metrics.precision_score(y_test, rf_predictions))\n",
        "\n",
        "# Model Recall: what percentage of positive tuples are labelled as such?\n",
        "print(\"Recall:\",metrics.recall_score(y_test, rf_predictions))\n",
        "\n",
        "#Printing Confusion Matrix\n",
        "conf_matrix = metrics.confusion_matrix(y_test, rf_predictions)\n",
        "print(\"Confusion Matrix: \", conf_matrix )\n",
        "\n",
        "#Printing F1 Score\n",
        "f1score = f1_score(y_test, rf_predictions, average='binary')\n",
        "print(\"F1 score: \", f1score)\n",
        "\n",
        "#Printing Cohen-Kappa score\n",
        "ckscore = cohen_kappa_score(y_test, rf_predictions)\n",
        "print(\"Cohen-Kappa Score: \", ckscore)\n",
        "\n",
        "#Printing Precision-Recall Curve\n",
        "average_precision = average_precision_score(y_test, rf_predictions)\n",
        "disp = plot_precision_recall_curve(model, X_test, y_test)\n",
        "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
        "                   'AP={0:0.2f}'.format(average_precision))\n",
        "\n",
        "\n",
        "#Printing ROC Curve\n",
        "metrics.plot_roc_curve(model, X_test, y_test)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#P-value\n",
        "_, _, p_value = permutation_test_score(model, X_test, y_test, scoring=\"neg_root_mean_squared_error\", cv=None, n_permutations=200)\n",
        "print(p_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIMIJ5dZacwM"
      },
      "source": [
        "###SVM Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6qg-NrNajYG"
      },
      "source": [
        "from sklearn import svm\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#Create a svm Classifier\n",
        "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
        "#Train the model using the training sets\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6A4v0VCalp0"
      },
      "source": [
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "\n",
        "# Model Accuracy: how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Model Precision: what percentage of positive tuples are labeled as such?\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
        "\n",
        "# Model Recall: what percentage of positive tuples are labelled as such?\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
        "\n",
        "#Printing Confusion Matrix\n",
        "conf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix: \", conf_matrix )\n",
        "\n",
        "#Printing F1 Score\n",
        "f1score = f1_score(y_test, y_pred, average='binary')\n",
        "print(\"F1 score: \", f1score)\n",
        "\n",
        "#Printing Cohen-Kappa score\n",
        "ckscore = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Cohen-Kappa Score: \", ckscore)\n",
        "\n",
        "#Printing Precision-Recall Curve\n",
        "average_precision = average_precision_score(y_test, y_pred)\n",
        "disp = plot_precision_recall_curve(clf, X_test, y_test)\n",
        "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
        "                   'AP={0:0.2f}'.format(average_precision))\n",
        "\n",
        "\n",
        "#Printing ROC Curve\n",
        "metrics.plot_roc_curve(clf, X_test, y_test)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#P-value\n",
        "_, _, p_value = permutation_test_score(clf, X_test, y_test, scoring=\"neg_root_mean_squared_error\", cv=None, n_permutations=200)\n",
        "print(p_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5K-5-BY-rY1"
      },
      "source": [
        "#Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZmb_qIM-uPy"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.tree import export_graphviz\n",
        "from IPython.display import Image \n",
        "from pydot import graph_from_dot_data\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GQ6Zo6sMa8X"
      },
      "source": [
        "y_pred_dt = dt.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw_8p_07-u85"
      },
      "source": [
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "\n",
        "# Model Accuracy: how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_dt))\n",
        "\n",
        "# Model Precision: what percentage of positive tuples are labeled as such?\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred_dt))\n",
        "\n",
        "# Model Recall: what percentage of positive tuples are labelled as such?\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred_dt))\n",
        "\n",
        "#Printing Confusion Matrix\n",
        "conf_matrix = metrics.confusion_matrix(y_test, y_pred_dt)\n",
        "print(\"Confusion Matrix: \", conf_matrix )\n",
        "\n",
        "#Printing F1 Score\n",
        "f1score = f1_score(y_test, y_pred_dt, average='binary')\n",
        "print(\"F1 score: \", f1score)\n",
        "\n",
        "#Printing Cohen-Kappa score\n",
        "ckscore = cohen_kappa_score(y_test, y_pred_dt)\n",
        "print(\"Cohen-Kappa Score: \", ckscore)\n",
        "\n",
        "#Printing Precision-Recall Curve\n",
        "average_precision = average_precision_score(y_test, y_pred_dt)\n",
        "disp = plot_precision_recall_curve(dt, X_test, y_test)\n",
        "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
        "                   'AP={0:0.2f}'.format(average_precision))\n",
        "\n",
        "\n",
        "#Printing ROC Curve\n",
        "metrics.plot_roc_curve(dt, X_test, y_test)\n",
        "plt.show()\n",
        "\n",
        "#P-value\n",
        "_, _, p_value = permutation_test_score(dt, X_test, y_test, scoring=\"neg_root_mean_squared_error\", cv=None, n_permutations=200)\n",
        "print(p_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ensemble Model"
      ],
      "metadata": {
        "id": "WhOxPafzqYzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import six\n",
        "import sys\n",
        "sys.modules['sklearn.externals.six'] = six\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "lr = LogisticRegression()\n",
        "clf_stack = StackingClassifier(classifiers =[clf, dt, model], meta_classifier = lr, use_probas = False, use_features_in_secondary = True)\n",
        "model_stack = clf_stack.fit(X_train, y_train)\n",
        "pred_stack = model_stack.predict(X_test)"
      ],
      "metadata": {
        "id": "rINbzNF5qXcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "\n",
        "# Model Accuracy: how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, pred_stack))\n",
        "\n",
        "# Model Precision: what percentage of positive tuples are labeled as such?\n",
        "print(\"Precision:\",metrics.precision_score(y_test, pred_stack))\n",
        "\n",
        "# Model Recall: what percentage of positive tuples are labelled as such?\n",
        "print(\"Recall:\",metrics.recall_score(y_test, pred_stack))\n",
        "\n",
        "#Printing Confusion Matrix\n",
        "conf_matrix_dt = metrics.confusion_matrix(y_test, pred_stack)\n",
        "print(\"Confusion Matrix: \", conf_matrix_dt )\n",
        "\n",
        "#Printing F1 Score\n",
        "f1score = f1_score(y_test, pred_stack, average='binary')\n",
        "print(\"F1 score: \", f1score)\n",
        "\n",
        "#Printing Cohen-Kappa score\n",
        "ckscore = cohen_kappa_score(y_test, pred_stack)\n",
        "print(\"Cohen-Kappa Score: \", ckscore)"
      ],
      "metadata": {
        "id": "cPRQ2XMLqm49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Forming Combined Graphs"
      ],
      "metadata": {
        "id": "c7j2CksniMmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure()\n",
        "\n",
        "from sklearn.metrics import plot_roc_curve\n",
        "classifiers = [clf, dt, model]\n",
        "ax = plt.gca()\n",
        "for i in classifiers:\n",
        "    plot_roc_curve(i, X_test, y_test, ax=ax)"
      ],
      "metadata": {
        "id": "EDwunJi_iPVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Precision Recall Curve\n",
        "\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "classifiers = [clf, dt, model]\n",
        "ax = plt.gca()\n",
        "for i in classifiers:\n",
        "    plot_precision_recall_curve(i, X_test, y_test, ax=ax)"
      ],
      "metadata": {
        "id": "QohYZLWfg-_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxOiWtW2yLNI"
      },
      "source": [
        "##MRI:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJS5fxSmyNy1"
      },
      "source": [
        "mri = mri.sort_values(by=['RID'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlRGS39ByRqm"
      },
      "source": [
        "###Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6KDqnomyT5T"
      },
      "source": [
        "sns.catplot(x='VISCODE', y='Hippocampus', data=mri, hue=\"DX_bl\", height=8.27, aspect=11.7/8.27)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRQolk15yUdI"
      },
      "source": [
        "sns.catplot(x='VISCODE', y='Entorhinal', data=mri, hue=\"DX_bl\", height=8.27, aspect=11.7/8.27)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdRGWOcpyWfA"
      },
      "source": [
        "###Data formatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYssGK74yYTQ"
      },
      "source": [
        "hippo = []\n",
        "entor = []\n",
        "labels = []\n",
        "start = 2\n",
        "rids_mri = []\n",
        "l1 = [0,0,0,0,0,0] #bl, m06, m12, m24, m36, m48 PTMEM\n",
        "l2 = [0,0,0,0,0,0] #bl, m06, m12, m24, m36, m48 PTTOT\n",
        "cnt = 0\n",
        "for index,row in mri.iterrows():\n",
        "  if start != row['RID']:    \n",
        "    if sum(l1) != 0 and sum(l2) != 0 and cnt>2:\n",
        "      rids_mri.append(row['RID'])\n",
        "      hippo.append(l1)\n",
        "      entor.append(l2)\n",
        "      if dec == 'AD':\n",
        "        labels.append(1)\n",
        "      else:\n",
        "        labels.append(0)\n",
        "    l1 = [0,0,0,0,0,0]\n",
        "    l2 = [0,0,0,0,0,0]\n",
        "    start = row['RID']\n",
        "    cnt=0\n",
        "  dec = row['DX_bl']\n",
        "  m = row['VISCODE']\n",
        "  hi = row['Hippocampus']\n",
        "  en = row['Entorhinal']\n",
        "  if m == 'bl':\n",
        "    cnt+=1\n",
        "    l1[0] = hi\n",
        "    l2[0] = en\n",
        "  elif m == 'm06':\n",
        "    cnt+=1\n",
        "    l1[1] = hi\n",
        "    l2[1] = en\n",
        "  elif m == 'm12':\n",
        "    cnt+=1\n",
        "    l1[2] = hi\n",
        "    l2[2] = en\n",
        "  elif m == 'm24':\n",
        "    cnt+=1\n",
        "    l1[3] = hi\n",
        "    l2[3] = en\n",
        "  elif m == 'm36':\n",
        "    cnt+=1\n",
        "    l1[4] = hi\n",
        "    l2[4] = en\n",
        "  elif m == 'm48':\n",
        "    cnt+=1\n",
        "    l1[5] = hi\n",
        "    l2[5] = en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iocBsSu9yxDI"
      },
      "source": [
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.datasets import make_classification\n",
        "from collections import Counter\n",
        "from numpy import where\n",
        "from matplotlib import pyplot\n",
        "# define dataset\n",
        "X_smote = np.concatenate((np.array(hippo), np.array(entor)), axis=1)\n",
        "y_smote = labels\n",
        "\n",
        "print(X_smote)\n",
        "print(y_smote)\n",
        "\n",
        "# summarize class distribution\n",
        "counter = Counter(y_smote)\n",
        "print(counter)\n",
        "\n",
        "# scatter plot of examples by class label\n",
        "for label, _ in counter.items():\n",
        "\trow_ix = where(y_smote == label)[0]\n",
        "\tpyplot.scatter(X_smote[row_ix, 0], X_smote[row_ix, 1], label=str(label))\n",
        "pyplot.legend()\n",
        "pyplot.show()\n",
        "\n",
        "# transform the dataset\n",
        "oversample = SMOTE(sampling_strategy=0.5)\n",
        "X_smote, y_smote = oversample.fit_resample(X_smote, y_smote)\n",
        "\n",
        "print(X_smote)\n",
        "print(y_smote)\n",
        "\n",
        "# summarize the new class distribution\n",
        "counter = Counter(y_smote)\n",
        "print(counter)\n",
        "\n",
        "# scatter plot of examples by class label\n",
        "for label, _ in counter.items():\n",
        "\trow_ix = where(y_smote == label)[0]\n",
        "\tpyplot.scatter(X_smote[row_ix, 0], X_smote[row_ix, 1], label=str(label))\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl16C5d8y2KX"
      },
      "source": [
        "newarr = np.array_split(X_smote, 2, axis=1)\n",
        "x1_mri = newarr[0]\n",
        "x2_mri = newarr[1]\n",
        "#Length of data in Cog\n",
        "print(x1_mri)\n",
        "print(len(x1_mri),len(x2_mri))\n",
        "y_mri = y_smote"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhSLOwUCydlQ"
      },
      "source": [
        "imputer = KNNImputer(n_neighbors=32, missing_values=0, weights = 'distance')\n",
        "x1_mri = imputer.fit_transform(x1_mri) #Hippocampus\n",
        "x2_mri = imputer.fit_transform(x2_mri) #Entorhinal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aahIODGrygPC"
      },
      "source": [
        "###Model_mri"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD5gqRn6yhun"
      },
      "source": [
        "print(\"Shape of input:\",len(x1_mri),len(x1_mri[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS1zLpOwylPo"
      },
      "source": [
        "#Reshaping for GRU\n",
        "x1_mri = x1_mri.reshape((1,1357,6))\n",
        "x2_mri = x2_mri.reshape((1,1357,6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tkfr8-vxymwa"
      },
      "source": [
        "#Normalizing all data\n",
        "x1_mri = x1_mri/2000\n",
        "x2_mri = x2_mri/2000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd0Jgl6SyoRJ"
      },
      "source": [
        "#Generating feature vectors\n",
        "inputs3 = Input(shape=(1357, 6))\n",
        "gru3 = GRU(6, return_sequences=True, return_state=True)\n",
        "whole_sequence_output, final_state = gru3(inputs3)\n",
        "model3 = Model(inputs=inputs3, outputs=whole_sequence_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QcEthc0yp-A"
      },
      "source": [
        "hippo_fv = model3.predict(x1_mri)\n",
        "entor_fv = model3.predict(x2_mri)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjOBfeEsaXvc"
      },
      "source": [
        "#Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw75_audzIuo"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# 70% training and 30% test\n",
        "hippo_1 = hippo_fv[0]\n",
        "entor_1 = entor_fv[0]\n",
        "mri_fv = np.concatenate((hippo_1, entor_1), axis=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(mri_fv, y_mri, test_size=0.3,random_state=109)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PdDbhmPabW2"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# Create the model with 100 trees\n",
        "model = RandomForestClassifier(n_estimators=100, \n",
        "                               bootstrap = True,\n",
        "                               max_features = 'sqrt')\n",
        "# Fit on training data\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq3hTRssad6Q"
      },
      "source": [
        "# Actual class predictions\n",
        "rf_predictions = model.predict(X_test)\n",
        "# Probabilities for each class\n",
        "rf_probs = model.predict_proba(X_test)[:, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP41FfI1afg8"
      },
      "source": [
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "\n",
        "# Model Accuracy: how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, rf_predictions))\n",
        "\n",
        "# Model Precision: what percentage of positive tuples are labeled as such?\n",
        "print(\"Precision:\",metrics.precision_score(y_test, rf_predictions))\n",
        "\n",
        "# Model Recall: what percentage of positive tuples are labelled as such?\n",
        "print(\"Recall:\",metrics.recall_score(y_test, rf_predictions))\n",
        "\n",
        "#Printing Confusion Matrix\n",
        "conf_matrix = metrics.confusion_matrix(y_test, rf_predictions)\n",
        "print(\"Confusion Matrix: \", conf_matrix )\n",
        "\n",
        "#Printing F1 Score\n",
        "f1score = f1_score(y_test, rf_predictions, average='binary')\n",
        "print(\"F1 score: \", f1score)\n",
        "\n",
        "#Printing Cohen-Kappa score\n",
        "ckscore = cohen_kappa_score(y_test, rf_predictions)\n",
        "print(\"Cohen-Kappa Score: \", ckscore)\n",
        "\n",
        "#Printing Precision-Recall Curve\n",
        "average_precision = average_precision_score(y_test, rf_predictions)\n",
        "disp = plot_precision_recall_curve(model, X_test, y_test)\n",
        "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
        "                   'AP={0:0.2f}'.format(average_precision))\n",
        "\n",
        "\n",
        "#Printing ROC Curve\n",
        "metrics.plot_roc_curve(model, X_test, y_test)\n",
        "plt.show()\n",
        "\n",
        "#P-value\n",
        "_, _, p_value = permutation_test_score(model, X_test, y_test, scoring=\"neg_root_mean_squared_error\", cv=None, n_permutations=100)\n",
        "print(p_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDb5Yb0gahi2"
      },
      "source": [
        "#SVM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbDcJDFLzOKw"
      },
      "source": [
        "from sklearn import svm\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#Create a svm Classifier\n",
        "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
        "#Train the model using the training sets\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1nRwJm_zU-_"
      },
      "source": [
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "\n",
        "# Model Accuracy: how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Model Precision: what percentage of positive tuples are labeled as such?\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
        "\n",
        "# Model Recall: what percentage of positive tuples are labelled as such?\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
        "\n",
        "#Printing Confusion Matrix\n",
        "conf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix: \", conf_matrix )\n",
        "\n",
        "#Printing F1 Score\n",
        "f1score = f1_score(y_test, y_pred, average='binary')\n",
        "print(\"F1 score: \", f1score)\n",
        "\n",
        "#Printing Cohen-Kappa score\n",
        "ckscore = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Cohen-Kappa Score: \", ckscore)\n",
        "\n",
        "#Printing Precision-Recall Curve\n",
        "average_precision = average_precision_score(y_test, y_pred)\n",
        "disp = plot_precision_recall_curve(clf, X_test, y_test)\n",
        "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
        "                   'AP={0:0.2f}'.format(average_precision))\n",
        "\n",
        "\n",
        "#Printing ROC Curve\n",
        "metrics.plot_roc_curve(clf, X_test, y_test)\n",
        "plt.show()\n",
        "\n",
        "#P-value\n",
        "_, _, p_value = permutation_test_score(clf, X_test, y_test, scoring=\"neg_root_mean_squared_error\", cv=None, n_permutations=200)\n",
        "print(p_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDTeAlJSMsG7"
      },
      "source": [
        "#Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UKQZR6mMuBv"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.tree import export_graphviz\n",
        "from IPython.display import Image \n",
        "from pydot import graph_from_dot_data\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6kAYTlEMznO"
      },
      "source": [
        "y_pred_dt = dt.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lrIArYEM5UX"
      },
      "source": [
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "\n",
        "# Model Accuracy: how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_dt))\n",
        "\n",
        "# Model Precision: what percentage of positive tuples are labeled as such?\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred_dt))\n",
        "\n",
        "# Model Recall: what percentage of positive tuples are labelled as such?\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred_dt))\n",
        "\n",
        "#Printing Confusion Matrix\n",
        "conf_matrix = metrics.confusion_matrix(y_test, y_pred_dt)\n",
        "print(\"Confusion Matrix: \", conf_matrix )\n",
        "\n",
        "#Printing F1 Score\n",
        "f1score = f1_score(y_test, y_pred_dt, average='binary')\n",
        "print(\"F1 score: \", f1score)\n",
        "\n",
        "#Printing Cohen-Kappa score\n",
        "ckscore = cohen_kappa_score(y_test, y_pred_dt)\n",
        "print(\"Cohen-Kappa Score: \", ckscore)\n",
        "\n",
        "#Printing Precision-Recall Curve\n",
        "average_precision = average_precision_score(y_test, y_pred_dt)\n",
        "disp = plot_precision_recall_curve(dt, X_test, y_test)\n",
        "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
        "                   'AP={0:0.2f}'.format(average_precision))\n",
        "\n",
        "\n",
        "#Printing ROC Curve\n",
        "metrics.plot_roc_curve(dt, X_test, y_test)\n",
        "plt.show()\n",
        "\n",
        "#P-value\n",
        "_, _, p_value = permutation_test_score(dt, X_test, y_test, scoring=\"neg_root_mean_squared_error\", cv=None, n_permutations=200)\n",
        "print(p_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ensemble Model"
      ],
      "metadata": {
        "id": "l9SpPhkbq7uF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import six\n",
        "import sys\n",
        "sys.modules['sklearn.externals.six'] = six\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "lr = LogisticRegression()\n",
        "clf_stack = StackingClassifier(classifiers =[clf, dt, model], meta_classifier = lr, use_probas = False, use_features_in_secondary = True)\n",
        "model_stack = clf_stack.fit(X_train, y_train)\n",
        "pred_stack = model_stack.predict(X_test)"
      ],
      "metadata": {
        "id": "4dFSJT0sq9qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "\n",
        "# Model Accuracy: how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, pred_stack))\n",
        "\n",
        "# Model Precision: what percentage of positive tuples are labeled as such?\n",
        "print(\"Precision:\",metrics.precision_score(y_test, pred_stack))\n",
        "\n",
        "# Model Recall: what percentage of positive tuples are labelled as such?\n",
        "print(\"Recall:\",metrics.recall_score(y_test, pred_stack))\n",
        "\n",
        "#Printing Confusion Matrix\n",
        "conf_matrix_dt = metrics.confusion_matrix(y_test, pred_stack)\n",
        "print(\"Confusion Matrix: \", conf_matrix_dt )\n",
        "\n",
        "#Printing F1 Score\n",
        "f1score = f1_score(y_test, pred_stack, average='binary')\n",
        "print(\"F1 score: \", f1score)\n",
        "\n",
        "#Printing Cohen-Kappa score\n",
        "ckscore = cohen_kappa_score(y_test, pred_stack)\n",
        "print(\"Cohen-Kappa Score: \", ckscore)"
      ],
      "metadata": {
        "id": "P2kIWkerrDKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Forming Combined Graphs"
      ],
      "metadata": {
        "id": "F6jBO5bOjLCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure()\n",
        "\n",
        "from sklearn.metrics import plot_roc_curve\n",
        "classifiers = [clf, dt, model]\n",
        "ax = plt.gca()\n",
        "for i in classifiers:\n",
        "    plot_roc_curve(i, X_test, y_test, ax=ax)"
      ],
      "metadata": {
        "id": "hReJwLh4jNOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Precision Recall Curve\n",
        "\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "classifiers = [clf, dt, model]\n",
        "ax = plt.gca()\n",
        "for i in classifiers:\n",
        "    plot_precision_recall_curve(i, X_test, y_test, ax=ax)"
      ],
      "metadata": {
        "id": "lGhK0QRJi8_x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}